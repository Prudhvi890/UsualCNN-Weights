{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad82f056-fe64-4f81-b74e-49329905b97e",
   "metadata": {},
   "source": [
    "# 1. Build your own convolutional neural network using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a65e4fd0-cd18-4d1e-b587-a93506f906b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UsualCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UsualCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UsualCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(1024 * 1 * 1, 512) \n",
    "        self.fc2 = nn.Linear(512, 3) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        x = self.pool(F.relu(self.conv6(x)))\n",
    "        x = self.pool(F.relu(self.conv7(x)))\n",
    "        x = x.view(-1, 1024 * 1 * 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "net = UsualCNN()\n",
    "\n",
    "# Print the network architecture\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d9e51-7c67-43c6-be00-265ac66a9700",
   "metadata": {},
   "source": [
    "# 2. Train your model using dog heart dataset (you may need to use  Google Colab (or Kaggle) with GPU to train your code) \n",
    "\n",
    "### (1) use torchvision.datasets.ImageFolder for the training dataset\n",
    "### (2) use custom dataloader for test dataset (return image tensor and file name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62d21e7e-c447-41ae-b38a-b14c65fe9c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Data transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Custom dataset for test images without class folders\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if fname.endswith(('jpg', 'png', 'jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder('C:/Users/mahar/Downloads/Dog_heart/Dog_heart/Train', transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder('C:/Users/mahar/Downloads/Dog_heart/Dog_heart/Valid', transform=val_transforms)\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = CustomTestDataset(\n",
    "    root_dir='C:/Users/mahar/Downloads/Test/Test',\n",
    "    transform=test_transforms\n",
    ")\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0759f56-7311-4bfa-8649-cb5a56413097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = UsualCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb144408-6259-4a40-9801-ec894edf0db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/45], Train Loss: 1.0137, Val Loss: 1.0029\n",
      "Saving model with validation loss 1.0029\n",
      "Epoch [2/45], Train Loss: 1.0078, Val Loss: 0.9847\n",
      "Saving model with validation loss 0.9847\n",
      "Epoch [3/45], Train Loss: 0.9958, Val Loss: 0.9888\n",
      "Epoch [4/45], Train Loss: 0.9872, Val Loss: 0.9677\n",
      "Saving model with validation loss 0.9677\n",
      "Epoch [5/45], Train Loss: 0.9724, Val Loss: 0.9379\n",
      "Saving model with validation loss 0.9379\n",
      "Epoch [6/45], Train Loss: 0.9385, Val Loss: 1.0712\n",
      "Epoch [7/45], Train Loss: 0.9622, Val Loss: 0.8814\n",
      "Saving model with validation loss 0.8814\n",
      "Epoch [8/45], Train Loss: 0.9343, Val Loss: 0.8053\n",
      "Saving model with validation loss 0.8053\n",
      "Epoch [9/45], Train Loss: 0.8985, Val Loss: 0.8878\n",
      "Epoch [10/45], Train Loss: 0.8946, Val Loss: 0.7427\n",
      "Saving model with validation loss 0.7427\n",
      "Epoch [11/45], Train Loss: 0.8724, Val Loss: 0.7416\n",
      "Saving model with validation loss 0.7416\n",
      "Epoch [12/45], Train Loss: 0.8684, Val Loss: 0.7360\n",
      "Saving model with validation loss 0.7360\n",
      "Epoch [13/45], Train Loss: 0.8407, Val Loss: 0.7769\n",
      "Epoch [14/45], Train Loss: 0.8769, Val Loss: 0.7423\n",
      "Epoch [15/45], Train Loss: 0.8309, Val Loss: 0.7352\n",
      "Saving model with validation loss 0.7352\n",
      "Epoch [16/45], Train Loss: 0.7998, Val Loss: 0.7094\n",
      "Saving model with validation loss 0.7094\n",
      "Epoch [17/45], Train Loss: 0.7964, Val Loss: 0.6706\n",
      "Saving model with validation loss 0.6706\n",
      "Epoch [18/45], Train Loss: 0.8229, Val Loss: 0.6796\n",
      "Epoch [19/45], Train Loss: 0.8087, Val Loss: 0.7062\n",
      "Epoch [20/45], Train Loss: 0.7973, Val Loss: 0.6763\n",
      "Epoch [21/45], Train Loss: 0.7609, Val Loss: 0.6751\n",
      "Epoch [22/45], Train Loss: 0.7976, Val Loss: 0.7064\n",
      "Epoch [23/45], Train Loss: 0.7543, Val Loss: 0.6337\n",
      "Saving model with validation loss 0.6337\n",
      "Epoch [24/45], Train Loss: 0.7496, Val Loss: 0.6628\n",
      "Epoch [25/45], Train Loss: 0.7549, Val Loss: 0.6894\n",
      "Epoch [26/45], Train Loss: 0.7479, Val Loss: 0.6954\n",
      "Epoch [27/45], Train Loss: 0.7311, Val Loss: 0.6432\n",
      "Epoch [28/45], Train Loss: 0.7129, Val Loss: 0.6557\n",
      "Epoch [29/45], Train Loss: 0.7074, Val Loss: 0.7076\n",
      "Epoch [30/45], Train Loss: 0.7254, Val Loss: 0.7035\n",
      "Epoch [31/45], Train Loss: 0.7037, Val Loss: 0.6516\n",
      "Epoch [32/45], Train Loss: 0.7497, Val Loss: 0.6557\n",
      "Epoch [33/45], Train Loss: 0.7110, Val Loss: 0.6676\n",
      "Epoch [34/45], Train Loss: 0.6984, Val Loss: 0.6901\n",
      "Epoch [35/45], Train Loss: 0.7054, Val Loss: 0.6611\n",
      "Epoch [36/45], Train Loss: 0.6874, Val Loss: 0.6274\n",
      "Saving model with validation loss 0.6274\n",
      "Epoch [37/45], Train Loss: 0.6817, Val Loss: 0.6383\n",
      "Epoch [38/45], Train Loss: 0.7002, Val Loss: 0.6414\n",
      "Epoch [39/45], Train Loss: 0.6903, Val Loss: 0.6237\n",
      "Saving model with validation loss 0.6237\n",
      "Epoch [40/45], Train Loss: 0.6850, Val Loss: 0.6759\n",
      "Epoch [41/45], Train Loss: 0.6987, Val Loss: 0.6952\n",
      "Epoch [42/45], Train Loss: 0.6699, Val Loss: 0.6738\n",
      "Epoch [43/45], Train Loss: 0.6737, Val Loss: 0.6825\n",
      "Epoch [44/45], Train Loss: 0.6745, Val Loss: 0.6842\n",
      "Epoch [45/45], Train Loss: 0.6473, Val Loss: 0.6721\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 45\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train model\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Save the model if the validation loss decreased\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'UsualCnn_model.pth')\n",
    "        print(f'Saving model with validation loss {val_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bab9c27-159c-49c9-93bd-91152bd7152b",
   "metadata": {},
   "source": [
    "# 3. Evaluate your model using the developed software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ee5bf4a-a6b0-49cb-b572-d9f7b960aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model.load_state_dict(torch.load('C:/Users/mahar/UsualCnn_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "test_predictions = []\n",
    "image_paths = []\n",
    "with torch.no_grad():\n",
    "    for images, paths in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_predictions.extend(predicted.cpu().numpy())\n",
    "        image_paths.extend(paths)\n",
    "\n",
    "# Predictions to save in a CSV file\n",
    "def save_predictions_to_csv(predictions, filenames, filename):\n",
    "    filenames = [os.path.basename(path) for path in filenames]\n",
    "    df = pd.DataFrame(list(zip(filenames, predictions)))\n",
    "    df.to_csv(filename, index=False, header=False)\n",
    "\n",
    "save_predictions_to_csv(test_predictions, image_paths, 'UsualCnn_test_predictions.csv')"
   ]
  },
  {
   "attachments": {
    "6a8f1ab5-5244-44af-ac90-ad64acd8c7e6.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "532b487b-351a-4bbb-b6b4-0d4e16f432e9",
   "metadata": {},
   "source": [
    "![image.png](attachment:6a8f1ab5-5244-44af-ac90-ad64acd8c7e6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae08c0c1-0af9-4f75-a9a9-783e8a94c87a",
   "metadata": {},
   "source": [
    "# 4. Compare results with [RVT paper](https://www.nature.com/articles/s41598-023-50063-x). Requirement: performance is better than VGG16: 70%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24840ff0-8340-4040-aa06-bb312487a8e6",
   "metadata": {},
   "source": [
    "The model was determined to be 70.5% accurate by the results obtained. These are the same performances as those reported in (RVTpaper) where a 70% accuracy is presented and VGG16 model were used. Thus, it can be said that the model performance is equivalent with VGG16 as mentioned in RVT paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc93859c-b7a9-4313-9ca6-730a3e07c555",
   "metadata": {},
   "source": [
    "# 5. Write a four-page paper report using the shared LaTex template. Upload your paper to ResearchGate or Arxiv, and put your paper link and GitHub weight link here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fa698b-98b5-40d0-956e-cb10a7242e99",
   "metadata": {},
   "source": [
    "ResearchGate Link: https://www.researchgate.net/publication/382111904_UsualCNN_A_Custom_Convolutional_Neural_Network_for_Image_Classification\n",
    "\n",
    "Github Link: https://github.com/Prudhvi890/UsualCNN-Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f67144-4066-4541-9d7f-5bc6432bdb6b",
   "metadata": {},
   "source": [
    "# 6. Grading rubric\n",
    "\n",
    "(1). Code ------- 20 points (you also need to upload your final model as a pt file)\n",
    "\n",
    "(2). Grammer ---- 20 points\n",
    "\n",
    "(3). Introduction & related work --- 10 points\n",
    "\n",
    "\n",
    "(4). Method  ---- 20 points\n",
    "\n",
    "(5). Results ---- 20 points\n",
    "\n",
    "     > = 70 % -->10 points\n",
    "     < 50 % -->0 points\n",
    "     >= 50 % & < 70% --> 0.5 point/percent\n",
    "     \n",
    "\n",
    "(6). Discussion - 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a53889-e667-426e-a162-e017ca0ae6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
